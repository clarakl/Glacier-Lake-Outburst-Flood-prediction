# -*- coding: utf-8 -*-
"""Clara Klinkenberg Bachelor Thesis Coding Component.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_OM-L8KoCmAGrZhzXm3eyJ4YDdXnrjo
"""

# Parts of the code that have been taken over from Christian Kienholz as-is or in an adapted form are marked as following:

#-----------------------------------
# Start
#
#         CODE
#
# End
#-----------------------------------

# These parts still might still have gone through change, but the framework stayed like the original

# Connecting to the drive


from google.colab import drive
drive.mount('/content/drive')

# Change the path to my current directory

import os

path = '/content/drive/MyDrive/Academia/Tilburg university/2023-2024/Bachelor thesis/Data'
os.chdir(path)

#-------------------------------------------
# Start


# Pre-processing of data before 2019

#%% Dataframe and stats-related functions

def sample_uniform(inputlist):
    [lowerlimit, upperlimit] = inputlist
    return float((np.random.uniform(lowerlimit, upperlimit, 1)))

def sample_gaussian(inputlist):
    [mu, sigma] = inputlist
    num = np.random.standard_normal(1)
    return float((num*sigma + mu))

def percentile(n):
    def percentile_(x):
        return np.percentile(x, n)
    percentile_.__name__ = 'percentile_%s' % n
    return percentile_

def timeseries_to_minutes(series, method='linear', order=3):
    '''
    resample timeseries to minutes and linearly interpolate for nans
    '''
    series = series.resample('T').mean()
    if method != 'polynomial':
        series = series.interpolate(method=method)
    else:
        series = series.interpolate(method=method, order=order)
    return series

def time_to_delta_hours(series):
    '''
    convert datetime into delta hours since start time.
    scipy curve fit does not work with times, hence conversion is needed
    '''

    delta_time = (series.index - series.index[0]).tolist()
    delta_hours = np.array([t.total_seconds() for t in delta_time]) /  (60.0 ** 2)

    return delta_hours

def delta_hours_to_time(delta_hours, start_time):
    '''
    convert delta hours since start time back into datetimes
    '''

    timeobject = [start_time + dt.timedelta(hours=hour) for hour in list(delta_hours)]

    return timeobject

def rolling_mean_df(df, elements=15):

# take rolling mean to smooth out curves, rolling mean over 15 elements
    df_rm = df.rolling(elements, center=True).mean()

    # replace the nans at the end of the series with rolling means over fewer elements
    for loc in range(1, int(elements / 2) + 1):
        loc2 = 2 * loc - 1
        df_rm.iloc[-loc] = df[-loc2:].rolling(loc2, center=True).mean().iloc[-loc]

    # replace the nans at the beginning of the series with rolling means over fewer elements
    for loc in range(0, int(elements / 2)):
        loc2 = 2 * loc + 1
        df_rm.iloc[loc] = df[0:loc2 + 1].rolling(loc2, center=True).mean().iloc[loc]

    return df_rm

#%% Plotting-related functions

def clean_up_legend(ax):
    handles, labels = ax.get_legend_handles_labels()
    # remove duplicate lables and handles
    by_label = OrderedDict(zip(labels, handles))
    # sort both labels and handles by labels
    labels, handles = zip(*sorted(by_label.items(), key=lambda t: t[0]))

    return [handles, labels]

def format_date_axis(plt, ax, kw, rot=45, yr=10, sbyr=1):
    from matplotlib.dates import YearLocator
    from matplotlib.dates import MonthLocator
    from matplotlib.dates import DayLocator
    from matplotlib.dates import WeekdayLocator
    from matplotlib.dates import DateFormatter
    from matplotlib.dates import MO

    if kw == 'tenyear_year':
        years10 = YearLocator(yr)
        years = YearLocator(sbyr)
        Fmt = DateFormatter('%Y')

        # format the ticks
        ax.xaxis.set_major_locator(years10)
        ax.xaxis.set_major_formatter(Fmt)
        ax.xaxis.set_minor_locator(years)

    if  kw == 'months':
        years = YearLocator(1)
        months = MonthLocator()
        Fmt = DateFormatter('%m/%y')
        ax.xaxis.set_major_locator(months)
        ax.xaxis.set_major_formatter(Fmt)

    if  kw == 'weeks':
        weeks = WeekdayLocator(byweekday = MO, interval=1)
        days = DayLocator() # every month
        Fmt = DateFormatter('%m/%d')
        ax.xaxis.set_major_locator(weeks)
        ax.xaxis.set_major_formatter(Fmt)
        ax.xaxis.set_minor_locator(days)

    if  kw == 'days':
        years = YearLocator(1)
        months = MonthLocator()
        days = DayLocator()
        Fmt = DateFormatter('%m/%d')
        ax.xaxis.set_major_locator(days)
        ax.xaxis.set_major_formatter(Fmt)

    if  kw == 'day_month_year':
        years = YearLocator(1)
        months = MonthLocator()
        days = DayLocator()
        Fmt = DateFormatter('%m/%d/%y')
        ax.xaxis.set_major_locator(days)
        ax.xaxis.set_major_formatter(Fmt)

    labels = ax.get_xticklabels()
    plt.setp(labels, rotation=rot)

def minorlabeling(ax, ticks, side):
    if side == 'x':
        ax.set_xticks((ticks), minor = 1)
        ax.tick_params('x', length=2, width=.5, which='minor')
    elif side == 'y':
        ax.set_yticks((ticks), minor = 1)
        ax.tick_params('y', length=2, width=.5, which='minor')

def annotatefun(ax, textlist, xstart, ystart, ydiff=0.05, fonts=12, col='k', ha='left'):
    counter = 0
    for textstr in textlist:
        ax.text(xstart, (ystart - counter * ydiff), textstr, fontsize=fonts,
                ha=ha, va='center', transform=ax.transAxes, color=col)
        counter += 1

def digit_formatter(inputnumber, digits):
    return ("{0:0.%sf}"%(digits)).format(round(inputnumber, digits) + 0.0)


import pandas as pd
import numpy as np
import pickle


#%% Variables and helper functions

feet_to_m = 0.3048
feet3_to_m3 = feet_to_m ** 3 # 1 cubic feet = 0.0283168 m3
inch_to_mm = 25.4
m_to_feet = 3.28084

# load lookup tables to convert between runoff and lake elevation and vice versa
runoff_vs_elev = pd.read_csv('runoff_vs_elev.csv')
elev_vs_runoff = pd.read_csv('elev_vs_runoff.csv')
outflow_vs_gageheight = pd.read_csv("Outflow_Gage_Height_Lookup_Table.csv")

elev_vs_area_mendenhall = pd.read_csv('mendenhall_area_lookup.csv')
elev_vs_volume_mendenhall = pd.read_csv("elev_vs_volume_mendenhall.csv")

def foot_to_runoff(foot, elev_vs_runoff):
    meter = foot * feet_to_m
    return elev_vs_runoff[round(meter, 3)]

def runoff_to_foot(m3ps, runoff_vs_elev):
    foot = runoff_vs_elev[round(m3ps, 2)] * m_to_feet
    return foot


def calc_inflow(runoff_mendenhall_series, level_mendenhall_series,
                lake_area=elev_vs_area_mendenhall, smoothing=0, smoothing_len=60):
    '''
    Function to calculate discharge into Mendenhall Lake from outflow and lake level
    dV / dt = Q_in - Q_out
    smoothing_length is in minutes
    '''

    runoff_mendenhall = np.array(runoff_mendenhall_series)
    mendenhall_level = np.array(level_mendenhall_series)

    # determine time_diff in minutes
    time_diff = int((runoff_mendenhall_series.index[1] - runoff_mendenhall_series.index[0]).total_seconds() / 60.0)

    runoff0 = runoff_mendenhall[0:-1]
    runoff1 = runoff_mendenhall[1:]
    level0 = mendenhall_level[0:-1]
    level1 = mendenhall_level[1:]

    # calculates level change from one minute to the next
    delta_level = level1 - level0

    # create array with lake areas as function of lake level
    lake_area = np.array(np.round(lake_area, 2))

    levels = lake_area[:, 1]  # Levels
    areas = lake_area[:, 0]  # Corresponding Areas

    # Interpolate areas for each level1 value
    lake_area = np.interp(level1, levels, areas)

    # convert the area of Mendenhall lake from sqkm to m2,
    # divide by sampling_interval (minutes) * 60 to get m3 per second
    deltaV = delta_level * lake_area * 1000 * 1000 / (time_diff * 60)

    # mean outflow at minute level
    mean_outflow = (runoff0 + runoff1) / 2.0

    mean_inflow = deltaV + mean_outflow

    mean_inflow = np.squeeze(mean_inflow)
    if mean_inflow.ndim > 1:
        mean_inflow = mean_inflow[:, 0]  # Take all rows of the first column if still two-dimensional

    mendenhall_inflow = pd.Series(mean_inflow, index=np.squeeze(level_mendenhall_series.index[1:]))
    mendenhall_inflow.name = 'mendenhall_inflow'

    if int(smoothing_len / time_diff) < 1:
        smoothing = 0

    if smoothing == 0:
        pass
    else:
        mendenhall_inflow = rolling_mean_df(mendenhall_inflow, elements=int(smoothing_len / time_diff))
        mendenhall_inflow.name = 'mendenhall_inflow'

    # Resample to hourly frequency
    mendenhall_inflow_hourly = mendenhall_inflow.resample('H').mean()

    return mendenhall_inflow_hourly



def interpolate_area(df, target_height):
    '''
    Helper function to interpolate the area for a given target height.
    '''
    # Find the two closest contour levels
    lower_bound = df[df['Contour'] <= target_height].max()
    upper_bound = df[df['Contour'] >= target_height].min()

    # Linearly interpolate the area value
    interpolated_area = (lower_bound['Area'] +
                         (upper_bound['Area'] - lower_bound['Area']) *
                         (target_height - lower_bound['Contour']) /
                         (upper_bound['Contour'] - lower_bound['Contour']))
    return interpolated_area

def calc_suicide_volume(start_height, end_height):
    '''
    Function to calculate the water volume in Suicide Basin as a function of
    water height at the beginning and end of drainage.
    df_lookup is the lookup table created with the create_lookup_table_SuiB_volume
    function.
    '''
    df_lookup = pd.read_csv('suicide_volume_lookup_2019.csv')

    bin_height = 0.1
    volume = 0
    elev_range = np.arange(float(end_height), float(start_height), bin_height)

    end_values = list(elev_range[1:])
    start_values = list(elev_range[:-1])

    # loop over elevation range and integrate volume
    for start_value in start_values:

        # Use the helper function to interpolate the area
        interpolated_area = interpolate_area(df_lookup, start_value)

        # Multiply the interpolated area by the bin_height to get the volume slice
        volume += interpolated_area * bin_height * 1000**2 # converting area to m^2

    # convert volume to km^3
    volume /= 1000**3
    return volume


import urllib.request
import json
import pandas as pd
import datetime as dt
import scipy.optimize
from prophet import Prophet
from datetime import datetime

import matplotlib
import matplotlib.pyplot as plt

import numpy as np
import scipy
import matplotlib.dates as mdates

from bs4 import BeautifulSoup

import pickle

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


matplotlib.style.use('classic')

#%% Classes to download/prepare water level and discharge data

class Mendenhall_Lake_Gauge_Weather(object):
    '''
    Class to download Mendenhall runoff and gauge height from waterservices.usgs.gov
    '''

    def __init__(self):

        self.series_runoff = None
        self.series_gage_height = None
        self.feet_to_m = 0.3048
        self.feet3_to_m3 = 0.0283168  # cubic feet to cubic meters


    def json_dict_to_series(self, json_dict, name):
        '''
        Method to convert json dictionary into a time series
        '''

        value = []
        datetime = []

        for item in json_dict:

            try:
                datetime.append(dt.datetime.strptime(item['dateTime'],
                                                     '%Y-%m-%dT%H:%M:%S.000-08:00'))
            except:
                datetime.append(dt.datetime.strptime(item['dateTime'],
                                                     '%Y-%m-%dT%H:%M:%S.000-09:00'))
            if name == 'gage_height':
                value.append(float(item['value']) * self.feet_to_m)
            elif name == 'runoff':
                value.append(float(item['value']) * self.feet3_to_m3)
            else:
                raise ValueError('keyword {} not recoginized'.format(name))

        # create time series
        series = pd.Series(value, index=datetime, name=name)

        return series

    def download(self, days=1780):

        # created using https://waterservices.usgs.gov/rest/DV-Test-Tool.html PT50H
        # downloads runoff and water level
        url = ('http://waterservices.usgs.gov/nwis/iv/?format=json&sites=15052500&period'
               '=P{}D&parameterCd=00020,00045,00060,00065&siteStatus=all').format(days) # ,00065

        service = urllib.request.urlopen(url) # open the URL
        json_str = service.read() # read into json string

        # convert json string to dictionary
        # returns a list of dataframes with time and value (runoff or gage height)
        json_dict_runoff = json.loads(json_str)['value']['timeSeries'][2]['values'][0]['value']
        json_dict_wlevel = json.loads(json_str)['value']['timeSeries'][3]['values'][0]['value']

        self.series_runoff = self.json_dict_to_series(json_dict_runoff, 'runoff')
        self.series_gage_height = self.json_dict_to_series(json_dict_wlevel, 'gage_height')

#%%


class Mendenhall_Lake_GLOF_Model_Parameters(object):
    '''
    Class that provides the input parameters for Mendenhall_Lake_GLOF_Model.
    Adapt parameters as needed.
    '''

    def __init__(self):

        self.mc = 1 # switch for monte carlo simulations with hybrid model yes (1) or no (0)


        # create instance to download gauge and weather data from the usgs server
        Mendenhall_series = Mendenhall_Lake_Gauge_Weather()

        Mendenhall_series.download(days=20) # download data from the last 10 days

        self.series_runoff_menden = Mendenhall_series.series_runoff
        self.series_gage_height_menden = Mendenhall_series.series_gage_height

        self.glof_start = pd.to_datetime('2024-05-01 10:00:00')
        self.current_time = self.series_runoff_menden.index[-1]


        if self.mc == 1:
            print("With Monte Carlo")
            # how many monte carlo simulations to run
            self.mc_nr = 2
            # how many hours of the inflow time series to use to fit the exponential
            self.hours_overlap_range = [5, 10]
            # how much of the water drains during the rising limb (0.74 is 74%)
            self.percentage_rising = [0.74, 0.96]
            # vary the derived slope of the inflow curve within this value, 0.2 = 20%
            self.slope_change = 0.2
            # apply maximum slope change at this time (hours)
            self.slope_change_time = [48, 72]
            # by how much to vary the glof volume (0.1 is +-10%)
            self.glof_volume_var = 0.1
            # by how much to vary the baseflow (0.3 is +-30%)
            self.baseflow_var = 0.3

        else:
            print("Without Monte Carlo")
            self.mc_nr = 1
            self.hours_overlap = 10
            self.percentage_rising = 0.78

        # for how many days to run the model
        self.model_end = self.glof_start + dt.timedelta(days=20)

        # load lookup tables to convert between discharge and Mendenhall
        # Lake elevation and vice versa
        self.runoff_vs_elev = pd.read_csv('runoff_vs_elev.csv')
        self.elev_vs_runoff = pd.read_csv('elev_vs_runoff.csv')
        self.outflow_vs_gageheight = pd.read_csv("Outflow_Gage_Height_Lookup_Table.csv")

        # load lookup table that contains the Mendenhall Lake areas and volumes as a function of the water level
        self.elev_vs_area_mendenhall = pd.read_csv('mendenhall_area_lookup.csv')

        self.elev_vs_volume_mendenhall = pd.read_csv('elev_vs_volume_mendenhall.csv')
        self.elev_vs_volume_mendenhall = self.elev_vs_volume_mendenhall[["Elevation(meters)", "Volume(m^3)"]]

        # switch for log y axis (1) or not (0) on final plots
        self.log = 1

        # x-extent for final plots
        self.fig_mindate = self.current_time.date() - dt.timedelta(days=5)
        self.fig_maxdate = self.current_time.date() + dt.timedelta(days=5)


class Mendenhall_Lake_GLOF_Model(Mendenhall_Lake_GLOF_Model_Parameters):
    '''
    Class predicting the water inflow into Mendenhall Lake for a given GLOF volume
    and baseflow. The predicted inflow curve portion is derived from the most
    recent observed slope of the inflow curve, by fitting an exponential.
    Several parameters are varied as part of the MC calculations.
    The outflow from Mendenhall Lake is calculated via storage function.
    '''

    def __init__(self):

        # inherit the model parameters from the parameter class
        Mendenhall_Lake_GLOF_Model_Parameters.__init__(self)

        # rolling mean, resample to minutes, and linear interpolation for nans
        # discharge
        outflow_for_inflow_forecast = full_data[["datetime", "outflow"]]
        outflow_for_inflow_forecast["datetime"] = pd.to_datetime(outflow_for_inflow_forecast["datetime"])
        outflow_for_inflow_forecast.set_index("datetime", inplace=True)
        outflow_for_inflow_forecast = outflow_for_inflow_forecast["outflow"]
        self.series_runoff_rm = rolling_mean_df(outflow_for_inflow_forecast, elements=15)
        self.series_runoff_rm = timeseries_to_minutes(self.series_runoff_rm, method='linear')

        # gauge height
        gageheight_for_inflow_forecast = full_data[["datetime", "gage_height_m"]]
        gageheight_for_inflow_forecast["datetime"] = pd.to_datetime(gageheight_for_inflow_forecast["datetime"])
        gageheight_for_inflow_forecast.set_index("datetime", inplace=True)
        gageheight_for_inflow_forecast_series = gageheight_for_inflow_forecast["gage_height_m"]


        series_gage_height_rm = rolling_mean_df(gageheight_for_inflow_forecast_series, elements=15)
        series_gage_height_rm = timeseries_to_minutes(series_gage_height_rm, method='linear')

        # # cut the last two hours (smoothing doesn't work well here)
        cutoff_time = self.current_time - dt.timedelta(hours=2)
        self.fit_end_time = cutoff_time


        # Calculate historical inflow based on physical measurements of runoff and lake levels
        self.inflow_series = calc_inflow(self.series_runoff_rm , series_gage_height_rm,lake_area=self.elev_vs_area_mendenhall, smoothing=1,smoothing_len=120)
        # cut glof inflow series at the glof start time
        self.inflow_series_glof = self.inflow_series[self.glof_start:]


        # code to create a baseflow series
        # set first baseflow value to the value of the inflow_series at the glof_start time
        self.current_time = self.inflow_series.index[-1]
        self.base_inflow_start = self.inflow_series[self.glof_start]
        self.base_inflow_end = self.inflow_series[self.current_time]

        # create a constant baseflow curve from scratch
        # additional values can be added between first and last value if needed
        date_list = [self.glof_start, self.model_end]
        self.flow_list = [self.base_inflow_start, self.base_inflow_end]

        # convert date strings to datetime
        self.date_list = [pd.to_datetime(date) for date in date_list]

        # create a copy of the original baseflow list (to be used in the MC simulations)
        self.flow_list_orig = np.array(self.flow_list).copy()

    def runoff_fit_fun_mc(self, t, a, b):
        '''
        Function contains exponential to be fitted to data.
        Function also varies the slope of the curve.
        Adaptation of the factor is implemented as linear function,
        with no change at t = 0 and maximum change between 48 and 72 hours.
        '''

        # random sampling for slope increase 1 + x is multiplied with actual slope
        pool_1 = np.arange(-self.slope_change, self.slope_change, 0.01)
        choice_1 = np.random.choice(pool_1)

        # random sampling for time
        pool_2 = np.arange(self.slope_change_time[0], self.slope_change_time[1])
        choice_2 = np.random.choice(pool_2)

        # factor implemented as a linear function
        factor = 1 + (t - self.hours_overlap) * (choice_1 / choice_2)
        adapted =  factor * a * np.exp(b * t)

        # make sure curve is continuous
        # subtract value at transition (i.e, at time == overlap_hours)
        diff = factor * a * np.exp(b * self.hours_overlap) - a * np.exp(b * self.hours_overlap)

        return adapted - diff


    def extend_runoff_series(self):
        '''
        Function to extend the inflow series into the future, using an
        exponential fit to the data
        '''

        start_time = self.fit_end_time - dt.timedelta(hours=self.hours_overlap)

        # isolate the inflow series portion on which to conduct the curve fitting
        series_sel = self.inflow_series_glof[start_time:self.fit_end_time]

        # convert to delta hours (scipy.optimize.curve_fit does not work with
        # datetimes)
        delta_hours = time_to_delta_hours(series_sel)

        # exponential to be fitted to data
        runoff_fit_fun = lambda t, a, b: a * np.exp(b * t)

        # calculate the curve_fit
        popt, pcov = scipy.optimize.curve_fit(runoff_fit_fun, delta_hours, series_sel)

        # extend delta hours into the future
        extension_time = 5 * 24
        delta_hours_ext = np.arange(delta_hours.max(), delta_hours.max() + extension_time, delta_hours[1] - delta_hours[0])

        # convert delta hours back to datetimes
        time_index = delta_hours_to_time(delta_hours_ext, start_time=series_sel.index[0])

        if self.mc == 1:
            discharge = self.runoff_fit_fun_mc(delta_hours_ext, popt[0], popt[1])
        else:
            discharge = runoff_fit_fun(delta_hours_ext, popt[0], popt[1])

        series_extended = pd.Series(discharge, index=time_index)

        return series_extended


    def forecast_inflow(self):

        print("Starting forecast_inflow...")

        series_extended = self.extend_runoff_series()
        # Append the predicted inflow series to the measured series
        series_extended_concat = pd.concat([self.inflow_series_glof[:self.fit_end_time], series_extended.iloc[1:]])
        series_extended_concat.name = 'discharge'

        # Check if the extended inflow series or the baseflow series is longer
        if series_extended.index[-1] < self.baseflow_series.index[-1]:
            baseflow_series = self.baseflow_series[:series_extended.index[-1]]

        else:
            # Extend baseflow series by adding the baseflow end value
            baseflow_series_extension = series_extended_concat[self.baseflow_series.index[-1]:][1:]
            # baseflow_series_extension[:] = self.baseflow_series[-1]
            baseflow_series = pd.concat([self.baseflow_series, baseflow_series_extension], axis=0)

        # Create runoff cumsum and subtract cumsum of base runoff
        series_extended_concat_cumsum = series_extended_concat.cumsum() - baseflow_series.cumsum()
        series_extended_concat_cumsum.name = 'discharge_cumsum'

        # Create dataframe with discharge and cumsum discharge
        series_concat = pd.concat([series_extended_concat, series_extended_concat_cumsum], axis=1)

        # Allocate a certain percentage of the GLOF volume to the rising limb
        if self.mc == 1:
            percentage = sample_uniform(self.percentage_rising)
        else:
            percentage = self.percentage_rising

        glof_volume_up = percentage * self.glof_volume

        # Identify peak discharge and time of peak discharge
        # peak_discharge = series_concat.loc[((series_concat['discharge_cumsum']) * 60 <= glof_volume_up), 'discharge'][-1]
        peak_discharge = max(baseflow_series)
        glof_volume_up_rl = series_concat.loc[((series_concat['discharge_cumsum']) * 60 <= glof_volume_up), 'discharge_cumsum'][-1] * 60
        peak_time = series_concat.loc[((series_concat['discharge_cumsum']) * 60 <= glof_volume_up)].index[-1]


        glof_volume_down = self.glof_volume - glof_volume_up_rl

        if peak_time < self.fit_end_time:
            print("forecast_inflow was not successful")
            return [None, 0]
        else:
            baseflow_mean_prior = baseflow_series[peak_time]
            flow_diff = abs(peak_discharge - baseflow_mean_prior)
            minutes_remaining = round((glof_volume_down * 2.0) / (flow_diff * 60), 0)

            volume_difference = 10 ** 10
            volume_diff_prev = volume_difference + 10
            n = 0

            while n < 100:  # Allow up to 100 iterations
                time_intersect = peak_time + pd.Timedelta(minutes=minutes_remaining)

                # Ensure time_intersect is after peak_time
                if time_intersect <= peak_time:
                    print(f"Invalid time_intersect: {time_intersect} <= {peak_time}")
                    break

                series_concat_sel = series_concat.loc[(series_concat.index <= peak_time) | (series_concat.index >= time_intersect)]['discharge']
                series_concat_sel[time_intersect:] = baseflow_series[time_intersect:]

                series_concat_sel = timeseries_to_minutes(series_concat_sel, method='linear')

                # Check if the series are empty
                if series_concat_sel.empty or baseflow_series[peak_time:time_intersect].empty:
                    print(f"Empty series between {peak_time} and {time_intersect}")
                    break

                volume_difference = glof_volume_down - 60 * (series_concat_sel[peak_time:time_intersect].cumsum().iloc[-1] - baseflow_series[peak_time:time_intersect].cumsum().iloc[-1])

                if volume_difference > 0:
                    minutes_remaining += 1
                else:
                    minutes_remaining -= 1

                if abs(volume_difference) - volume_diff_prev >= 0:
                    time_intersect = time_intersect_prev
                    break

                volume_diff_prev = abs(volume_difference)
                time_intersect_prev = time_intersect

                n += 1

            if time_intersect <= peak_time:
                print(f"Final time_intersect is invalid: {time_intersect} <= {peak_time}")
                return [None, 0]

            time_intersect = peak_time + pd.Timedelta(minutes=minutes_remaining)

            series_concat = series_concat.loc[(series_concat.index <= peak_time) | (series_concat.index >= time_intersect)]['discharge']
            series_concat[time_intersect:] = baseflow_series[time_intersect:]

            time_limit = time_intersect + pd.Timedelta(days=5)
            series_concat_reduced = series_concat[:time_limit]

            series_concat_reduced = timeseries_to_minutes(series_concat_reduced, method='linear')

            series_concat_reduced_rm = rolling_mean_df(series_concat_reduced, elements=120)

            return series_concat_reduced_rm

# End
# ------------------------------------------------------------------

    def forecast_outflow_univariate_prophet(self):

        # Prepare the data for Prophet
        df = full_data[["datetime", "outflow"]]
        df["datetime"] = pd.to_datetime(df["datetime"])
        df = df.rename(columns={"datetime": "ds", "outflow": "y"})
        df.dropna(inplace=True)
        df.set_index('ds', inplace=True)
        df = df.resample('h').mean()
        df['y'] = df['y'].clip(lower=0)
        df.reset_index(inplace=True)

        # Define the split date
        split_date = '2020-04-01 00:00:00'

        # Split the data into training and test sets
        train_data = df[df['ds'] < split_date]
        test_data = df[df['ds'] >= split_date]

        # Grid search setup
        param_grid = {
                  'changepoint_prior_scale': [0.01, 0.1, 0.5],
                  'n_changepoints': [20, 25, 30],
                  'changepoint_range': [0.8, 0.9, 0.95],
        }

        # Generate all combinations of parameters
        all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
        best_score = float('inf')
        best_params = None

        for index, params in enumerate(all_params):
          m = Prophet(changepoint_prior_scale=params['changepoint_prior_scale'],
                      n_changepoints = params['n_changepoints'],
                      changepoint_range=params['changepoint_range'],)
          m.fit(train_data)
          future = m.make_future_dataframe(periods=len(test_data), freq='H')
          fcst = m.predict(future)

          # Calculate metrics
          mae = mean_absolute_error(test_data['y'], fcst[-len(test_data):]['yhat'])
          mse = mean_squared_error(test_data['y'], fcst[-len(test_data):]['yhat'])
          rmse = np.sqrt(mse)
          r2 = r2_score(test_data['y'], fcst[-len(test_data):]['yhat'])

          score = mse  

          if score < best_score:
              best_score = score
              best_params = params
              best_forecast = fcst

          print(f"Gridsearch {index + 1} of {len(all_params)} with params {params} done. Current best score: {best_score}")

        # Refit model with the best parameters
        best_model = Prophet(changepoint_prior_scale=params['changepoint_prior_scale'],
                        n_changepoints = params['n_changepoints'],
                        changepoint_range=params['changepoint_range'],)
        best_model.fit(train_data)

        print('Best Parameters:', best_params)
        print(f'Best Scores: MAE: {mae:.3f}, MSE: {mse:.3f}, RMSE: {rmse:.3f}, R2: {r2:.3f}')

        # Plot test set predictions 
        plt.figure(figsize=(14, 7))
        plt.plot(test_data['ds'], test_data['y'], 'b-', label='Actual')
        plt.plot(test_data['ds'], best_forecast[-len(test_data):]['yhat'], 'r--', label='Predicted')
        plt.title('Test Set Predictions', fontsize=14)
        plt.xlabel('Date', fontsize=12)
        plt.ylabel('Outflow', fontsize=12)
        plt.legend(fontsize=12)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        plt.tight_layout()
        plt.show()

        # Create future dataframe for predictions
        future_data = best_model.make_future_dataframe(periods=4400, freq='h')

        # Predict using the model
        forecast_data = best_model.predict(future_data)

        forecast_data[["ds", "yhat", "yhat_lower", "yhat_upper"]].tail(5)

        forecast_data.set_index('ds', inplace=True)
        forecast_data = pd.Series(forecast_data["yhat"], index=forecast_data.index)

        # Save the forecast DataFrame as a pickle file
        with open('multivariate_forecast_outflow_prophet.pickle', 'wb') as f:
            pickle.dump(forecast_data, f)
            print("multivariate_forecast_outflow_prophet.pickle successfully saved")

        return forecast_data


    def forecast_outflow_multivariate_prophet(self, inflow_data):

        # There will be no printing or plotting the evaluation metrics because
        # The mutlivariate version of Prophet will run through multiple Monte Carlo runs
        # That would be too much output
        # Once the Monte carlo runs are finished, the mean values for the metrics will be printed.


        # Prepare the data for Prophet
        df = full_data[["datetime", "outflow"]]
        df["datetime"] = pd.to_datetime(df["datetime"])
        df = df.rename(columns={"datetime": "ds", "outflow": "y"})
        df.dropna(inplace=True)
        df.set_index('ds', inplace=True)
        df = df.resample('h').mean()
        df['y'] = df['y'].clip(lower=0)
        df.reset_index(inplace=True)


        # Prepare regressor

        # Ensure inflow_data has the right format
        inflow_data = inflow_data.rename("inflow_y")
        inflow_data = inflow_data.reset_index().rename(columns={"datetime": "ds"})
        inflow_data = inflow_data.dropna()  # Drop NaN values in inflow_data

        # Merge df with inflow data to include inflow_y as a regressor
        df = pd.merge(df, inflow_data, how='left', on='ds')
        df['inflow_y'].fillna(method='ffill', inplace=True)  # Forward fill NaN values
        df['inflow_y'].fillna(method='bfill', inplace=True)  # Backward fill NaN values

        # Define the split date
        split_date = '2020-04-01 00:00:00'

        # Split the data into training and test sets
        train_data = df[df['ds'] < split_date]
        test_data = df[df['ds'] >= split_date]

        # Grid search setup
        param_grid = {
           'changepoint_prior_scale': [0.01, 0.1, 0.5],
            'n_changepoints': [20, 25, 30],
            'changepoint_range': [0.8, 0.9, 0.95],
            # 'seasonality_prior_scale': [0.1, 1.0, 10.0],
            # 'seasonality_mode': ['additive', 'multiplicative']
        }

        # Generate all combinations of parameters
        all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
        best_score = float('inf')
        best_params = None

        for index, params in enumerate(all_params):
          m = Prophet(uncertainty_samples=100,
                      changepoint_prior_scale=params['changepoint_prior_scale'],
                      n_changepoints = params['n_changepoints'],
                      changepoint_range=params['changepoint_range'],)
          m.add_regressor('inflow_y')
          m.fit(train_data)
          future = m.make_future_dataframe(periods=len(test_data), freq='H')
          # Merge the future dataframe with inflow data
          future = pd.merge(future, inflow_data, on='ds', how='left')

          # Ensure there are no missing values in 'inflow_y'
          future['inflow_y'].fillna(method='ffill', inplace=True)
          future['inflow_y'].fillna(method='bfill', inplace=True)

          fcst = m.predict(future)

          # Calculate metrics
          mae = mean_absolute_error(test_data['y'], fcst[-len(test_data):]['yhat'])
          mse = mean_squared_error(test_data['y'], fcst[-len(test_data):]['yhat'])
          rmse = np.sqrt(mse)
          r2 = r2_score(test_data['y'], fcst[-len(test_data):]['yhat'])

          score = mse  # You can change the scoring rule depending on which metric you prioritize

          if score < best_score:
              best_score = score
              best_params = params
              best_forecast = fcst

          print(f"Gridsearch {index + 1} of {len(all_params)} with params {params} done. Current best score: {best_score}")

        # Refit model with the best parameters
        best_model = Prophet(uncertainty_samples=100,
                              changepoint_prior_scale=params['changepoint_prior_scale'],
                              n_changepoints = params['n_changepoints'],
                              changepoint_range=params['changepoint_range'],)
        best_model.add_regressor('inflow_y')
        best_model.fit(train_data)

        print('Best Parameters:', best_params)
        print(f'Best Scores: MAE: {mae:.3f}, MSE: {mse:.3f}, RMSE: {rmse:.3f}, R2: {r2:.3f}')

        # Plot test set predictions with larger font
        plt.figure(figsize=(14, 7))
        plt.plot(test_data['ds'], test_data['y'], 'b-', label='Actual')
        plt.plot(test_data['ds'], best_forecast[-len(test_data):]['yhat'], 'r--', label='Predicted')
        plt.title('Test Set Predictions', fontsize=14)
        plt.xlabel('Date', fontsize=12)
        plt.ylabel('Outflow', fontsize=12)
        plt.legend(fontsize=12)
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=10)
        plt.tight_layout()
        plt.show()

        # Create future dataframe for predictions
        future_data = best_model.make_future_dataframe(periods=4400, freq='h')
        future_data = pd.merge(future_data, inflow_data, how='left', on='ds')
        future_data['inflow_y'].fillna(method='ffill', inplace=True)  # Forward fill NaN values
        future_data['inflow_y'].fillna(method='bfill', inplace=True)  # Backward fill NaN values

        # Predict using the model
        forecast_data = best_model.predict(future_data)

        # Step 1: Identify the overlapping period
        actual_end_date = df['ds'].max()  # The last timestamp of your actual data
        forecast_start_date = forecast_data['ds'].min()  # The first timestamp in your forecast data

        if actual_end_date < forecast_start_date:
            print("No overlapping data to evaluate. All forecast data is in the future.")
        else:
            # Step 2: Trim the forecast data to the actual data period for evaluation
            forecast_trimmed = forecast_data[forecast_data['ds'] <= actual_end_date]

            # Step 3: Compute evaluation metrics
            common_index = df['ds'][df['ds'].isin(forecast_trimmed['ds'])]
            df_common = df.set_index('ds').loc[common_index]
            forecast_common = forecast_trimmed.set_index('ds').loc[common_index]

            mse = mean_squared_error(df_common['y'], forecast_common['yhat'])
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(df_common['y'], forecast_common['yhat'])
            r2 = r2_score(df_common['y'], forecast_common['yhat'])

            # Optionally plot the actual data and the corresponding forecast
            plt.figure(figsize=(10, 5))
            plt.plot(df_common.index, df_common['y'], label='Actual Data')
            plt.plot(forecast_common.index, forecast_common['yhat'], label='Forecast', linestyle='--', color='red')
            plt.legend()
            plt.title('Actual Data vs Forecast - Multivariate time-series analysis - MC run 1')
            plt.show()

            print("")
            print(f"forecast_outflow_prophet MSE: {mse}")
            print(f"forecast_outflow_prophet RMSE: {rmse}")
            print(f"forecast_outflow_prophet MAE: {mae}")
            print(f"forecast_outflow_prophet R^2 Score: {r2}")
            print("")

        # Plot the full forecast with proper titles and labels
        fig1 = model.plot(forecast_data)
        ax1 = fig1.gca()
        ax1.set_title('Outflow Forecast')
        ax1.set_xlabel('Datetime')
        ax1.set_ylabel('Outflow (m^3/s)')
        plt.show()

        # Plot the forecast components with proper titles and labels
        fig2 = model.plot_components(forecast_data)
        for ax in fig2.axes:
            ax.set_xlabel('Datetime')
        fig2.axes[0].set_title('Trend forecast outflow')
        fig2.axes[0].set_ylabel('Outflow (m^3/s)')
        fig2.axes[1].set_title('Weekly Seasonality Forecast Outflow')
        fig2.axes[1].set_ylabel('Outflow (m^3/s)')
        fig2.axes[2].set_title('Yearly Seasonality Forecast Outflow')
        fig2.axes[2].set_ylabel('Outflow (m^3/s)')
        plt.show()

        forecast_data[["ds", "yhat", "yhat_lower", "yhat_upper"]].tail(5)

        forecast_data.set_index('ds', inplace=True)
        forecast_data = pd.Series(forecast_data["yhat"], index=forecast_data.index)

        # Save the forecast DataFrame as a pickle file
        with open('multivariate_forecast_outflow_prophet.pickle', 'wb') as f:
            pickle.dump(forecast_data, f)
            print("multivariate_forecast_outflow_prophet.pickle successfully saved")

        return forecast_data

# ----------------------------------------------------------
# Start
    

    def mc_calcs(self, water_vol):

        self.water_vol = water_vol

        # Empty lists for the Monte Carlo simulation results
        self.inflow_list = []
        self.outflow_list = []
        self.baseflow_list = []
        self.outflow_max_list = []
        self.outflow_max_time_list = []
        self.mc_mae = []
        self.mc_rmse = []
        self.mc_r2 = []

        if self.mc == 0:
            self.mc_nr = 1
            glof_volumes = [self.water_vol * 1000 ** 3] # Convert GLOF volume to m3/sec
            print(f"Running a single deterministic simulation with volume: {self.water_vol} m^3")
            self.baseflow_series = pd.Series(self.flow_list_orig, self.date_list, name='flow')
            self.baseflow_series = timeseries_to_minutes(self.baseflow_series, method='linear')
            # Make sure the baseflow_series does not start before the GLOF begins
            if self.baseflow_series.index.min() < self.glof_start:
                raise ValueError('Baseflow series ({}) starts before GLOF ({}), adapt baseflow '\
                                'accordingly'.format(self.baseflow_series.index.min(), self.glof_start))
            self.baseflow_list = [self.baseflow_series]

        else:
            glof_volumes = [sample_uniform([self.water_vol - self.water_vol * self.glof_volume_var,
                                            self.water_vol + self.water_vol * self.glof_volume_var]) * 1000 ** 3 for r in range(0, self.mc_nr)]
            print(f"Running {self.mc_nr} Monte Carlo simulations with volume variability around: {self.water_vol} m^3")

        # Simulation counter for reporting progress
        mc_range = range(0, self.mc_nr)
        self.mc_counter = 0

        for i, self.glof_volume in zip(mc_range, glof_volumes):
            if self.mc == 1:
                print(f"Simulation {i + 1}/{self.mc_nr}:")
                self.hours_overlap = sample_uniform(self.hours_overlap_range)
                print(f"  Hours overlap set to: {self.hours_overlap} hours")

                # Vary the baseflow during the MC simulation
                adapt_value = sample_uniform([1 - self.baseflow_var, 1 + self.baseflow_var])
                print(f"  Baseflow variation factor: {adapt_value}")

                # Adapt the baseflow values except the first one (which is known)
                for c in range(1, len(self.flow_list)):
                    self.flow_list[c] = self.flow_list_orig[c] * adapt_value
                self.baseflow_series = pd.Series(self.flow_list, self.date_list, name='flow')
                if self.baseflow_series.index.min() < self.glof_start:
                    raise ValueError('Baseflow series ({}) starts before GLOF ({}), adapt baseflow '\
                                    'accordingly'.format(self.baseflow_series.index.min(), self.glof_start))

                # Resample to minutes
                self.baseflow_series = timeseries_to_minutes(self.baseflow_series, method='linear')
                self.baseflow_list.append(self.baseflow_series)


            # Fit the inflow trend and predict future inflow
            fit_start_time = self.fit_end_time - dt.timedelta(hours=self.hours_overlap)
            self.inflow_series_trend_calc = self.inflow_series[fit_start_time:self.fit_end_time]

            self.inflow_forecasted = self.forecast_inflow()

            self.inflow_list.append(self.inflow_forecasted)

            if self.mc == 1:
                outflow_forecasted = self.forecast_outflow_multivariate_prophet(self.inflow_series)
            else:
                outflow_forecasted = self.forecast_outflow_univariate_prophet()


            self.outflow_list.append(outflow_forecasted)

            # Add the maximum outflow and its time
            if self.mc == 0:
                pd.to_datetime(outflow_forecasted["ds"])
                now = pd.to_datetime(self.current_time)
                filtered_series = outflow_forecasted[outflow_forecasted["ds"] >= now]
                filtered_series = filtered_series[filtered_series["ds"] <= self.current_time]
                self.outflow_max_list.append(filtered_series.max())
                self.outflow_max_time_list.append(filtered_series.idxmax())
            else:
                outflow_forecasted.index = pd.to_datetime(outflow_forecasted.index)
                now = pd.to_datetime(self.current_time)
                filtered_series = outflow_forecasted[outflow_forecasted.index >= now]
                filtered_series = filtered_series[filtered_series.index <= self.current_time]
                self.outflow_max_list.append(filtered_series.max())
                self.outflow_max_time_list.append(filtered_series.idxmax())

            # Cut baseflow series back to the actual GLOF
            self.baseflow_series_glof = self.baseflow_series[self.glof_start:self.model_end]

            self.mc_counter += 1

            # Evaluation Metrics for the current simulation
            if self.mc == 1:
                try:
                    common_index = full_data.set_index('datetime')["outflow"].index.intersection(outflow_forecasted.index)
                    actual_outflow = full_data.set_index('datetime')["outflow"].loc[common_index]
                    predicted_outflow = outflow_forecasted.loc[common_index]

                    mse = mean_squared_error(actual_outflow, predicted_outflow)
                    rmse = np.sqrt(mse)
                    mae = mean_absolute_error(actual_outflow, predicted_outflow)
                    r2 = r2_score(actual_outflow, predicted_outflow)

                    self.mc_mae.append(mae)
                    self.mc_rmse.append(rmse)
                    self.mc_r2.append(r2)

                    print(f"Simulation {i + 1}/{self.mc_nr} Metrics:")
                    print(f"MSE: {mse}")
                    print(f"RMSE: {rmse}")
                    print(f"MAE: {mae}")
                    print(f"R^2 Score: {r2}")

                    if i == 10:
                        # Plot actual vs predicted outflows for the current simulation
                        plt.figure(figsize=(14, 7))
                        plt.plot(actual_outflow.index, actual_outflow, label='Actual Outflow', color='blue')
                        plt.plot(predicted_outflow.index, predicted_outflow, label='Predicted Outflow', color='orange', linestyle='--')
                        plt.title(f'Actual vs Predicted Outflow - Simulation 1')
                        plt.xlabel('Time')
                        plt.ylabel('Outflow (m^3/s)')
                        plt.legend()
                        plt.grid(True)
                        plt.show()

                except Exception as e:
                    print(f"Error in evaluation or plotting: {e}")

                print("Monte Carlo simulations completed.")

                if i in range(0, self.mc_nr + 1, 100):
                    print('{} out of {} done...'.format(i, self.mc_nr))

        if self.mc == 1:
            # Summarize the Monte Carlo simulation results
            print("Monte Carlo Simulation Summary:")
            mean_mae = np.mean(self.mc_mae)
            mean_rmse = np.mean(self.mc_rmse)
            mean_r2 = np.mean(self.mc_r2)
            std_mae = np.std(self.mc_mae)
            std_rmse = np.std(self.mc_rmse)
            std_r2 = np.std(self.mc_r2)

            print(f"Mean MAE: {mean_mae}")
            print(f"Std MAE: {std_mae}")
            print(f"Mean RMSE: {mean_rmse}")
            print(f"Std RMSE: {std_rmse}")
            print(f"Mean R^2 Score: {mean_r2}")
            print(f"Std R^2 Score: {std_r2}")

            # Plot the distribution of metrics
            plt.figure(figsize=(14, 7))
            plt.hist(self.mc_mae, bins=20, alpha=0.7, label='MAE')
            plt.hist(self.mc_rmse, bins=20, alpha=0.7, label='RMSE')
            plt.title('Distribution of Evaluation Metrics - Monte Carlo Simulations')
            plt.xlabel('Metric Value')
            plt.ylabel('Frequency')
            plt.legend()
            plt.grid(True)
            plt.show()

            # Create a summary table
            summary_table = pd.DataFrame({
                'Metric': ['Mean MAE', 'Std MAE', 'Mean RMSE', 'Std RMSE', 'Mean R^2', 'Std R^2'],
                'Value': [mean_mae, std_mae, mean_rmse, std_rmse, mean_r2, std_r2]
            })

            print("\nMonte Carlo Simulation Summary Table:")
            print(summary_table)
        else:
          summary_table = []

        return self.mc_mae, self.mc_rmse, self.mc_r2, summary_table

# End
# --------------------------------------------------------------------------------

# Preprocessing for the 2019-2024 data
# This chunk utilizes the download function of the mendenhall model class,
# which is why it has to be run after the classes have been initialized
# and not in the beginning with the rest of the pre-processing code

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline

class DataProcessor:
    def __init__(self, series):
        self.series = series
        self.runoff = None
        self.gage_height = None

    def download_data(self, days):
        self.series.download(days=days)
        self.runoff = self.series.series_runoff.clip(lower=0)
        self.gage_height = self.series.series_gage_height.clip(lower=0)

    def check_duplicates(self):
        print(self.runoff.index.duplicated().sum())
        print(self.gage_height.index.duplicated().sum())

    def resample_data(self, freq='15min'):
        self.runoff = self.runoff.resample(freq).mean()
        self.gage_height = self.gage_height.resample(freq).mean()

    def merge_data(self):
        merged_data = pd.concat([self.runoff, self.gage_height], axis=1)
        merged_data.columns = ['Runoff', 'Gage Height']
        return merged_data

    def save_data(self, data, filename):
        data.to_csv(filename)

class RegressionModel:
    def __init__(self, filename):
        self.data = pd.read_csv(filename)

    def prepare_data(self):
        self.data = self.data[['time_AKST', 'inflow', 'outflow', 'gage_height_m']]
        self.data['time_AKST'] = pd.to_datetime(self.data['time_AKST'])
        self.data['inflow'] = self.data['inflow'].clip(lower=0)
        self.data['outflow'] = self.data['outflow'].clip(lower=0)
        self.data['gage_height_m'] = self.data['gage_height_m'].clip(lower=0)
        self.fill_missing_values()

    def fill_missing_values(self):
        start_date = self.data['time_AKST'].min()
        end_date = self.data['time_AKST'].max()
        complete_index = pd.date_range(start=start_date, end=end_date, freq='H')
        complete_data = pd.DataFrame(index=complete_index)
        self.data.set_index('time_AKST', inplace=True)
        self.data = complete_data.join(self.data, how='left')
        self.data['inflow'].fillna(self.data['inflow'].median(), inplace=True)
        self.data['outflow'].fillna(self.data['outflow'].median(), inplace=True)
        self.data['gage_height_m'].fillna(self.data['gage_height_m'].median(), inplace=True)

    def train_model(self):
        X = self.data[['outflow', 'gage_height_m']]
        y = self.data['inflow']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        pipeline = make_pipeline(SimpleImputer(strategy='mean'), LinearRegression())
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        linear_model = pipeline.named_steps['linearregression']
        coefficients = linear_model.coef_
        intercept = linear_model.intercept_
        print("Regression inflow MSE:", mse)
        print("Regression inflow R2 Score:", r2)
        print("Regression inflow Coefficients:", coefficients)
        print("Regression inflow Intercept:", intercept)
        return pipeline, linear_model

    def predict_new_data(self, new_data, linear_model):
        new_data.rename(columns={'Runoff': 'outflow', 'Gage Height': 'gage_height_m'}, inplace=True)
        new_data['outflow'].fillna(new_data['outflow'].median(), inplace=True)
        new_data['gage_height_m'].fillna(new_data['gage_height_m'].median(), inplace=True)
        predicted_inflow = linear_model.predict(new_data[['outflow', 'gage_height_m']])
        new_data['inflow'] = predicted_inflow
        new_data.to_csv("outflow_2019-2024.csv")
        return new_data

# Download recent data
series = Mendenhall_Lake_Gauge_Weather()
processor = DataProcessor(series)
processor.download_data(days=1780)
processor.check_duplicates()
processor.resample_data()
merged_data = processor.merge_data()
processor.save_data(merged_data, "downloaded_data.csv")

# Train regression model
model = RegressionModel("2011.04.01 - 2019.08.04 Gage_height_outflow_inflow_Mendenhall.csv")
model.prepare_data()
pipeline, linear_model = model.train_model()

# Predict new data
new_data = model.predict_new_data(merged_data[['Runoff', 'Gage Height']], linear_model)

# Merge and save final data
full_data = new_data.combine_first(model.data)
full_data.index = pd.to_datetime(full_data.index)
full_data = full_data.sort_index().reset_index().rename(columns={'index': 'datetime'})

# --------------------------------------------------------------
# Start

def main():

    # First, it needs to be decided
    # whether or not to run the Monte Carlo Simulations.
    # For this, go to the Mendenhall_Lake_GLOF_Model_Parameters class and use the switch self.mc in the init function

    # calculate water volume estimate for the upcoming GLOF event, based on
    # pre-GLOF water levels (measured from gauge and camera) and post-GLOF
    # water levels (estimated using data from previous events)
    water_vol = calc_suicide_volume(start_height=436, end_height=394)

#     create the forecaster object for the real-time forecasting
#     this loads the parameters defined in the class
#     "Mendenhall_Lake_GLOF_Model_Parameters" (check and adapt if needed)
    forecaster_automatic = Mendenhall_Lake_GLOF_Model()

      # run the model calculations
    forecaster_automatic.mc_calcs(water_vol)

    # post_process and plot the results
    forecaster_automatic.overfit_analysis(forecaster_automatic.inflow_series)

    # #retrieval of outflow timeseries
    tmp = forecaster_automatic.outflow_list
    print(tmp)

if __name__ == "__main__":

    main()

# End 
#-------------------------------------------------------
